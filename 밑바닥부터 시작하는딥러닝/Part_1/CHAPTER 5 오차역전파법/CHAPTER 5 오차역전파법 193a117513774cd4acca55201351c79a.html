<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>CHAPTER 5 오차역전파법</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="7b73e14b-1a6e-405e-abd9-ba104dadad20" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">🔸</span></div><h1 class="page-title">CHAPTER 5 오차역전파법</h1></header><div class="page-body"><p id="62ed9bee-d415-4baf-94c9-6818e44697cd" class="">
</p><nav id="fcfe50b1-99e9-4693-b3fe-12b1c0a023d0" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#f26ee907-0ec3-46fe-8dee-8e230665960b">1. 계산 그래프</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#0a9c114f-fcc4-4722-b58d-2e4dbc2baf2e">1.1 계산 그래프로 풀다</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#5a45a492-f801-44a8-87cb-f618bb85dd6b">1.2 국소적 계산</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#4f99a7f5-1d43-43d5-88d0-72f0584c7c09">1.3 왜 계산 그래프로 푸는가?</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#446c8a0b-d305-4f32-801a-62497f70df65">2. 연쇄법칙</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#c4219ecf-bc97-409f-893c-ac296f5e8055">2.1 계산 그래프의 역전파</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#9cef1a79-e8f8-49c8-a387-a5537e203f4e">2.2. 연쇄법칙이란</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#de2aeb93-20be-437e-a266-24d55d466bc6">2.3 연쇄법칙과 계산 그래프</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#fc3f8ce8-25a2-4e00-88aa-72b3503c3810">3. 역전파</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#90349b86-9993-493d-8fdc-61120ba3e03e">3.1 덧셈 노드의 역전파</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#6fda6591-cf9f-43a2-befc-9c2c5059e422">3.2 곱셉 노드의 역전파</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#fcaefef0-587d-4e90-aefe-e477b5ca080c">3.3 사과 쇼핑의 예</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#12a5f4fa-ecc6-4d92-89c7-3caf0c657676">4. 단순한 계층 구현하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#8d2ddcc4-88ca-43de-953e-b2d497754087">4.1 곱셈 계층</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#02caf145-8b9f-41b0-b2cb-b5d9fbd0123a">4.2 덧셈 계층</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#6e9049a7-b88b-4c97-8bfb-9f1907c1fb54">5. 활성화 함수 계층 구현하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#bc5d1032-8d99-4ef5-b682-b652c04f8d4a">5.1 ReLU 계층</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#a0aaf0f6-c84f-46f7-a9dd-186cec23ec54">5.2 Sigmoid 계층</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#f4677904-0240-4db4-b9bd-182905b56fac">6. Affine/Softmax 계층 구현하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#672b5511-28d1-43b5-b160-0e7d06ca6fe5">6.1 Affine 계층</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#6d928f4c-b71b-4455-bcc0-0ed5704b93d1">6.2 배치용 Affine 계층</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#88605a44-20c9-4b0d-9d71-72a2845624ab">6.3 Softmax-with-Loss 계층</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#d678d0e3-dd1d-4b76-b120-04d29aa11b19">7. 오차역전파법 구현하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#ef4a31c9-c7bb-4d90-83cb-e79e7cdd4a8a">7.1 신경망 학습의 전체 그림</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#a3d41d15-5a61-408e-96a9-8ab1c5334f44">7.2 오차역전파법을 적용한 신경망 구현하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#9bae0649-7218-4739-a41c-70bd1fbe2cd3">7.3 오차역전파법으로 구한 기울기 검증하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#d5a384dd-e89f-4783-94cb-988bc83cda4e">7.4 오차역전파법을 사용한 학습 구현하기</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#e049f463-69fa-401a-83d4-48bdc0ec47b3">8. 정리</a></div></nav><h1 id="f26ee907-0ec3-46fe-8dee-8e230665960b" class="">1. 계산 그래프</h1><p id="dae527f3-7f64-4f1a-a579-a83d66499bea" class="">계산 그래프: 계산과정을 그래프로 나타낸 것</p><p id="3b2347ec-bfa7-4aaa-bf10-b479975266fa" class="">그래프는 복수의 node와 edge로 표현됩니다.</p><p id="cdeb0c68-8a9d-40ec-be12-c9d6921fd7e4" class="">node사이의 직선을 edge라 합니다.</p><h2 id="0a9c114f-fcc4-4722-b58d-2e4dbc2baf2e" class="">1.1 계산 그래프로 풀다</h2><blockquote id="90ebd96f-62a6-4785-8f24-7c53e13a63d0" class="">문제1: 현빈군은 슈퍼에서 1개에 100원인 사과를 2개 샀다.                              이때 지불 금액을 구하라. 단, 소비세가 10% 부과된다. </blockquote><p id="44dc58ff-2238-45b5-9023-8981760629b7" class="">계산 그래프는 노드를 원(o)으로 표기하고 원 안에 연산 내용을 적는다. 또 계산 결과를 화살표 위에 적어 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해지게 한다. 위 문제를 계산 그래프로 풀면 다음과 같다.</p><p id="07aa6b87-43e7-44b7-b5c4-a676ae33b2c8" class="">
</p><figure id="193a1175-1377-4cd4-acca-55201351c79a" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled.png"><img style="width:806px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled.png"/></a></figure><p id="0c5c5da4-2d16-48bb-adff-d8a8cedb054d" class="">위 그림에서는 &#x27;x2&#x27;와 &#x27;x1.1&#x27;을 각각 하나의 연산으로 취급해 원 안에 표기했지만, 곱셉인 &#x27;x&#x27; 만을 연산으로 생각할 수 있다. 그 경우, 계산 그래프는 다음과 같이 표현할 수 있다.</p><p id="5834f5b7-4c43-487c-b99f-9b9dd86ba69c" class="">
</p><figure id="2cf4e980-13e1-43d5-8cd4-f80ae2a61a23" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%201.png"><img style="width:815px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%201.png"/></a></figure><p id="83e197fd-6ab2-49a4-81f0-ce755629c7c9" class="">
</p><blockquote id="9bb91586-c9e9-4106-8b55-994e9a03013d" class="">문제 2: 현빈군은 슈퍼에서 사과를 2개, 귤을 3개 샀다. 사과는 1개에 100원, 귤은 1개에 150원이다. 소비세가 10%일 때 지불 금액을 구하라</blockquote><p id="01186ef4-d6d4-45a8-854e-487ae3f86dc1" class="">위 문제를 계산 그래프로 풀면, 다음과 같이 그래프를 그릴 수 있다.</p><p id="1cc926e9-3d1b-457b-8e8c-57682ef41921" class="">
</p><figure id="fa1dca27-0451-4d4c-9890-3bd0497ee552" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%202.png"><img style="width:788px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%202.png"/></a></figure><p id="cae86838-fb7f-4e55-9742-0f5fefac85f2" class="">지금까지 살펴 본 것처럼, 계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행된다.</p><ol id="533f3b81-92a7-4e63-aed3-2e902a1d0f00" class="numbered-list" start="1"><li>계산그래프를 구성한다.</li></ol><ol id="dfb69941-9f54-4363-a63a-5f0741225cf6" class="numbered-list" start="2"><li>그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.<ul id="317ded9e-0c22-4f9b-917f-cf30625addf6" class="bulleted-list"><li>여기서 2번째 &#x27;계산을 왼쪽에서 오른쪽으로 진행&#x27; 하는 단계를 순전파라고 한다. 순전파는 계산 그래프의 출발점으로부터 종착점으로의 전파다. 반면, 오른쪽에서 왼쪽으로의 전파를 역전파라고 한다.</li></ul><p id="1b53117b-a25d-403e-adb7-c2994509a384" class="">
</p></li></ol><p id="3cf38736-8054-4fce-bbd6-8c8bcf89cffd" class="">
</p><h2 id="5a45a492-f801-44a8-87cb-f618bb85dd6b" class="">1.2 국소적 계산</h2><p id="670c45d1-325d-4c9a-a8ee-8561dbb23332" class="">계산 그래프의 특징은 &#x27;국소전 계산&#x27;을 전파함으로써 최종 결과를 얻는다는 점에 있다. 국소적 계산은 결국 전체에 어떤 일이 벌어지든 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다는 것이다. 예를 들어, 슈퍼에서 사과 2개를 포함한 여러 식품을 구입한다고 해 보자. 해당 계산 그래프는 당음과 같이 표현할 수 있다.</p><p id="b7d2c9c4-7cad-40b5-806d-879d541d3829" class="">
</p><figure id="9b51b7b9-6500-41fd-a25a-8bba47f0d3f5" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%203.png"><img style="width:815px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%203.png"/></a></figure><p id="b6e0851c-7b2a-47b2-bc0f-cae73e77f627" class="">여기에서 핵심은 각 노드에서의 계산이 국소적 계산이라는 것이다. 가령 사과와 그 외의 물품값을 더하는 계산은 4000이라는 숫자가 어떻게 계산되었느냐와는 상관없이, 단지 두 숫자를 더하면 된다는 것이다. 각 노드는 자신과 관련한계산(이 예에서는 입력된 두 숫자의 덧셈) 외에는 아무것도 신경 쓸 게 없다.</p><p id="3d62fb64-7e1d-4b14-9822-25024f699689" class="">
</p><p id="4be2c183-f6c6-40e9-8526-216cc59cf54d" class="">이처럼 계산 그래프는 국소적 계산에 집중한다. 전체 계산이 제아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 &#x27;국소적 계산&#x27;이다. 국소적인 계산은 단순하지만, 그 결과를 전달함으로써 전체를 구성하는 복잡한 계산을 해낼 수 있다.</p><p id="084dead1-c2a5-4b19-bc2a-833061ba4b90" class="">
</p><h2 id="4f99a7f5-1d43-43d5-88d0-72f0584c7c09" class="">1.3 왜 계산 그래프로 푸는가?</h2><p id="c63b6985-4fdd-4735-8e99-1e6b94650645" class="">계산 그래프의 이점이 뭘까? 하나는 방금 설명한 &#x27;국소적 계산&#x27;이다. 이는 전체가 복잡해도 각 노드에서 단순한 계산에 집중할 수 있게 해 문제를 단순화 시킬 수 있다.  다른 이점으로도, 계산 그래프는 중간 계산 결과를 모두 보관할 수 있다.</p><h1 id="446c8a0b-d305-4f32-801a-62497f70df65" class="">2. 연쇄법칙</h1><h2 id="c4219ecf-bc97-409f-893c-ac296f5e8055" class="">2.1 계산 그래프의 역전파</h2><p id="fdbca479-0c7c-4aed-b420-741e60268288" class="">
</p><p id="e1928241-c4e7-40b6-a909-392b0fc9d2b0" class="">위의 계산 그래프에서, 각 그래프의 edge(화살표)는 &#x27;국소적 미분&#x27;을 기억하고 있다. 이때, 이 국소적 미분을 전달하는 원리는 연쇄 법칙에 따른 것이다.</p><p id="b37d78f1-69ed-4372-b4c9-628388f257fb" class="">
</p><p id="0bf69c71-9397-4eaa-a6f2-4e26a2114353" class="">예를 들어, 다음과 같은 합성 함수가 있다고 가정해 보자. [z=t^2, t=x+y], 합성 함수란 여러 함수로 구성된 함수이다.</p><h2 id="9cef1a79-e8f8-49c8-a387-a5537e203f4e" class="">2.2. 연쇄법칙이란</h2><p id="f7bc75fa-9ff4-4045-8e8d-d273058b9c05" class="">
</p><p id="8ae40864-f01a-447b-a94f-c6f1b75ead56" class="">연쇄법칙은 합성 함수의 미분에 대한 성질이며, 다음과 같이 정의 된다.</p><p id="070f0174-a762-4024-a086-3ecd47614c7c" class="">
</p><ul id="fe3d0900-7f6f-4810-a62a-b85788142fa7" class="bulleted-list"><li>합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</li></ul><p id="db82fb78-db1d-43ba-b129-5ec693601145" class="">매우 간단한 성질이다! 예를 들어, 위의 식은</p><p id="97c059e5-bce3-4b43-aa81-a39e96958703" class="">
</p><figure id="70b0e21f-2d2b-4757-9137-9764d2231dee" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%204.png"><img style="width:243px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%204.png"/></a></figure><p id="27f8c89d-b669-4964-9941-267a9bc0e492" class="">이렇게 표현 할 수 있고, dt의 소거도 가능하다. 국소적 미분은 다음과 같다.</p><p id="9470cd51-b855-40f8-937a-9ca6f0958240" class="">
</p><figure id="ec7fa47e-1cbb-4e60-a882-c2f2da55e894" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%205.png"><img style="width:185px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%205.png"/></a></figure><p id="dffcce7a-f521-4492-9799-70125280dec0" class="">그러므로 dz/dx는 위의 두 미분을 곱해 계산 할 수 있다.</p><p id="bab5d0c6-4ddc-40af-ad6f-e939b0c38666" class="">
</p><figure id="cb56d252-ae84-4c77-bbea-a33a0709bd73" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%206.png"><img style="width:578px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%206.png"/></a></figure><p id="4e2080e4-fa7d-412c-b17b-06804688c6c4" class="">
</p><h2 id="de2aeb93-20be-437e-a266-24d55d466bc6" class="">2.3 연쇄법칙과 계산 그래프</h2><p id="0a6d6098-e719-4074-80ae-800641421bce" class="">위의 식을 계산 그래프로 나타내 보자 2제곱 계산을 &#x27;**2&#x27; 노드로 나타내면 다음과 같이 그릴 수 있다.</p><p id="b1fe3724-6cd4-4d6f-83bf-ede2ab794c80" class="">
</p><figure id="de1088d5-4c0e-4dd2-a7e6-c9fad57ee993" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%207.png"><img style="width:815px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%207.png"/></a></figure><p id="47b0826f-2a92-457c-88db-63ab41ebb086" class="">위와 같이 계산 그래프의 역전파는 오르쪽에서 왼쪽으로 신호를 전파한다. 맨 왼쪽을 주목하자, 이때 dz와 dt는 전부 소거되어 결국 남는 건 dz/dx가 된다. 이는 &#x27;x에 대한 z의 미분&#x27;을 의미한다.</p><p id="0afca5ca-eae9-4c1f-9510-93b36feaa694" class="">
</p><figure id="c6a33ada-99bd-41f3-8183-aa2039d4645e" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%208.png"><img style="width:806px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%208.png"/></a></figure><p id="753ea576-69b1-472d-bbe0-5a4932e2d1b9" class="">위의 그래프에 미분 값들을 대입하면 dz/dx가 2(x+y)임을 알 수 있다.</p><p id="a6daf466-ff16-4fc4-97ae-1d212feca549" class="">
</p><p id="9b2756c0-2bbf-4d6c-b191-b812c6141546" class="">
</p><p id="555d78aa-221d-41d9-ae8b-91c4d0235ede" class="">
</p><h1 id="fc3f8ce8-25a2-4e00-88aa-72b3503c3810" class="">3. 역전파</h1><p id="066ee414-399a-45b8-955c-8b9cbeecb72a" class="">앞 절에서는 계산 그래프의 역전파가 연쇄법칙에 따라 진행되는 모습을 보았다. 이번에는 &#x27;+&#x27;와 &#x27;x&#x27;등의 연산을 예로 들어 역전파의 구조를 살펴 보자.</p><h2 id="90349b86-9993-493d-8fdc-61120ba3e03e" class="">3.1 덧셈 노드의 역전파</h2><p id="c350cb90-cdbd-42c5-8bee-1a140b604f89" class="">z = x+y 라는 식이 있다고 해보자. 이때 미분은 다음과 같은 계산을 할 수 있다.</p><p id="da033820-e3b8-4675-abc1-2524817ad2e4" class="">dz/dx=1, dz/dy=1</p><p id="25e218a5-2dc1-4305-a716-a0d3daf675fa" class="">위 내용은 계산 그래프로 다음과 같이 표현 가능하다.</p><p id="e009e8bf-6286-40a6-96fa-e091e95bacab" class="">
</p><figure id="fd9ccbb4-1e63-46c1-bc78-011d06936b75" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%209.png"><img style="width:799px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%209.png"/></a></figure><p id="c90b342c-d9a5-4957-8aee-1e5e2bd76011" class="">위와 같이 역전파 때는 상류에서 전해진 미분에 1을 곱하여 하류로 흘린다. 즉, 덧셈 노드의 역전파는 1을 곱하기만 할 뿐이므로 입력된 값을 그대로 다음 노드로 보내게 된다.</p><p id="b5c603af-1827-4d77-8986-8d4711b70bf1" class="">
</p><figure id="d9555b4c-33d3-4905-92d3-a00dd9bafedb" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2010.png"><img style="width:800px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2010.png"/></a></figure><p id="ec85efea-7e55-464b-95dd-022e3a9f9502" class="">
</p><p id="b2a7f22c-694e-4c2a-8a84-f1dd1f71be1f" class="">dl/dz 라는 표현을 쓴 이유는, 위 그림과 같이 최종적으로 L이라는 값을 출력하는 큰 계산 그래프를 가정했기 때문이다.</p><p id="a9c7868f-28c0-4c2a-8f80-2565d0aaea9e" class="">
</p><p id="c3b9b1bd-69a2-4793-8f5e-79352b6a991a" class="">이제 구체적인 예를 하나 살펴보자. 가령 &#x27;10+5 =15&#x27;라는 계산이 있고 상류에서 1.3이라는 값이 흘러온다. 이를 계산 그래프로 그리면 다음 그림이 된다.</p><p id="b5ae47ef-8aba-429b-b62e-b167417c4321" class="">
</p><figure id="88372e7e-d73b-4cef-a55c-707d2b0e3475" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2011.png"><img style="width:803px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2011.png"/></a></figure><p id="1805798b-d8dc-4021-85c6-e7e82edcbc53" class="">덧셈 노드 역전파는 입력 신호를 다음 노드로 출력할 뿐이므로 위 그림처럼 1.3을 그대로 다음 노드로 전달한다.</p><p id="079b6a21-c688-4724-be60-6b88d64076b5" class="">
</p><h2 id="6fda6591-cf9f-43a2-befc-9c2c5059e422" class="">3.2 곱셉 노드의 역전파</h2><p id="fdc143c2-b278-45a7-a657-8e8e7f233d28" class="">
</p><p id="106f0e41-8822-48a1-af71-b6b7d03cea53" class="">이어서 곱셈 노드의 역전파를 살펴보자. z=xy라는 식이 있다고 가정하겠다. 이때, 계산 그래프는 다음과 같이 그려질 수 있다.</p><p id="bfe9d679-cd58-40e5-bf3f-c85d5a16c242" class="">
</p><figure id="ee31d3a0-e8e7-4056-a818-77651a3b3c27" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2012.png"><img style="width:798px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2012.png"/></a></figure><p id="ddafdcae-dc94-4b38-a045-0e54295fd466" class="">곱셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보낸다. 서로 바꾼 값이란 위 그림처럼 순전파때 x 였다면 역전파에서는 y. 순전파때 y였다면 역전파에서는 x로 바꾼다는 의미이다.</p><p id="6ce2e308-cd2f-44f1-8aea-d5dacf171c2b" class="">
</p><blockquote id="190df43f-c372-459a-8aed-69715e20bb6e" class="">&#x27;10 x 5 = 50&#x27;</blockquote><p id="ae84800f-d8ed-40c7-922e-71d6ec4b6501" class="">
</p><figure id="5f3ba9e3-617c-4085-82ff-c4c9deb32bf8" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2013.png"><img style="width:793px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2013.png"/></a></figure><p id="a0d4a1dd-731f-42cb-90ff-5d090b6dd62b" class="">덧셈의 역전파에서는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값이 필요하지 않았지만, 곱셈의 역전판느 순방향 입력 신호의 값이 필요하다. 그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해둔다.</p><p id="d2580cac-d1d4-4307-a1c3-4975006284d1" class="">
</p><p id="8543c93a-e550-417c-99e3-7d17e72c6393" class="">
</p><h2 id="fcaefef0-587d-4e90-aefe-e477b5ca080c" class="">3.3 사과 쇼핑의 예</h2><p id="9d8cb9fe-1125-4fe6-9ba6-fa3dbbcfec58" class="">
</p><figure id="69d638c0-7580-442d-9928-ba70881121a2" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2014.png"><img style="width:809px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2014.png"/></a></figure><h1 id="12a5f4fa-ecc6-4d92-89c7-3caf0c657676" class="">4. 단순한 계층 구현하기</h1><p id="0e2b6a97-511e-4381-8015-63a0bb6067dc" class="">이제 위에서 예를 들었던 &#x27;사과 쇼핑&#x27;의 예를 파이썬으로 구현해 보자. 계산 그래프의 곱셈노드를 &#x27;MulLayer&#x27;, 덧셈 노드를 &#x27;AddLayer&#x27;라는 이름으로 구현했다.</p><blockquote id="fe8e7e77-60c6-4a26-b8a4-a5cc9245b288" class="">신경망을 구성하는 &#x27;계층&#x27; 각각을 하나의 클래스로 구현하겠다. 여기에서 말하는 &#x27;계층&#x27;이란 신경망의 기능 단위이다</blockquote><h2 id="8d2ddcc4-88ca-43de-953e-b2d497754087" class="">4.1 곱셈 계층</h2><p id="9cb273f0-addb-4f77-b194-c98a6a838819" class="">모든 계층이 forward()와 backward()라는 공통의 메서드(인터페이스)를 갖도록 구현해보자. forward()는 순전파, backward()는 역전파를 처리한다.</p><p id="f1449802-fc36-4cfc-9774-8ab68fc83b3c" class="">
</p><p id="cebd4f52-ba15-4422-adf3-9f3e0915d0f8" class="">먼저 곱셈 계층을 구현 해 보자.</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="e27b9aaf-2e67-4c27-959c-7bf362682fff" class="code code-wrap"><code><span class="token keyword">class</span> <span class="token class-name">MulLayer</span><span class="token punctuation">:</span> 
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span> 
		self<span class="token punctuation">.</span>x <span class="token operator">=</span> <span class="token boolean">None</span> 
		self<span class="token punctuation">.</span>y <span class="token operator">=</span> <span class="token boolean">None</span> 
	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span> 
		self<span class="token punctuation">.</span>x <span class="token operator">=</span> x 
		self<span class="token punctuation">.</span>y <span class="token operator">=</span> y 
		out <span class="token operator">=</span> x <span class="token operator">*</span> y 
	<span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span> 
		dx <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>y
		dy <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>x 
		<span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy</code></pre><p id="aa71c068-246c-4746-ad3f-dbbc4689c9cd" class=""><strong>init()</strong>에서는 인스턴스 변수의 x와 y를 초기화 한다. 이 두 변수는 순전파 시의 입력 값을 유지하기 위해서 사용한다. forward() 에서는 x와 y를 인수로 받고 두 값을 곱해서 반환한다. 반면 forward()에서는 상류에서 넘어온 미분(dout)에 순전파 때의 값을 &#x27;서로 바꿔&#x27; 곱한 후 하류로 흘린다!</p><p id="46945a56-d02c-4fde-93eb-99a2d9cfbb7e" class="">
</p><figure id="eb043c82-7b8d-4ef4-939d-0547f39e8e6e" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2015.png"><img style="width:791px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2015.png"/></a></figure><p id="491124f4-72f6-45d4-9b05-ca090eb50fc9" class="">
</p><p id="da4d996a-f589-4605-801c-64a7f512ccba" class="">이제 실제 변수를 넣어 위의 그림을 구현 해 보자.</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="76e55f36-bc5f-474e-844a-2588bd247e46" class="code code-wrap"><code>apple <span class="token operator">=</span> <span class="token number">100</span>
apple_num <span class="token operator">=</span> <span class="token number">2</span>
tax <span class="token operator">=</span> <span class="token number">1.1</span> 
<span class="token comment">#계층들</span>
mul_apple_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> 
mul_tax_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> 
<span class="token comment">#순전파 </span>
apple_price <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple<span class="token punctuation">,</span> apple_num<span class="token punctuation">)</span> 
price <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple_price<span class="token punctuation">,</span> tax<span class="token punctuation">)</span> 

<span class="token keyword">print</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span> <span class="token comment"># 220</span>
</code></pre><p id="322911f3-e388-4cac-844c-0fb637f40d6a" class="">또, 각 변수에 대한 미분은 backward()에서 구할 수 있다.</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="04ddbbbd-c70d-40b7-833e-3ba57c9e501e" class="code code-wrap"><code><span class="token comment">#역전파 </span>
dprice <span class="token operator">=</span> <span class="token number">1</span> 
dapple_price<span class="token punctuation">,</span> dtax <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dprice<span class="token punctuation">)</span> 
dapple<span class="token punctuation">,</span> dapple_num <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dapple_price<span class="token punctuation">)</span> 

<span class="token keyword">print</span><span class="token punctuation">(</span>dapple<span class="token punctuation">,</span> dapple_num<span class="token punctuation">,</span> dtax<span class="token punctuation">)</span> <span class="token comment"># 2.2 110 200</span></code></pre><p id="1bb62380-9dce-4a4d-bac0-8f127a1831ed" class="">backward() 호출 순서는 forward()때와는 반대이다. 또 backward()가 받는 인수는 &#x27;순정파의 출력에 대한 미분&#x27;이다.</p><h2 id="02caf145-8b9f-41b0-b2cb-b5d9fbd0123a" class="">4.2 덧셈 계층</h2><p id="a8ae249e-9f81-4d36-aa16-e18b8a7fc111" class="">이어서 덧셈계층을 구현해 보자</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="df5bc965-3bf8-4c3d-b34f-a43694e46e3e" class="code code-wrap"><code><span class="token keyword">class</span> <span class="token class-name">AddLayer</span><span class="token punctuation">:</span> 
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span> 
		<span class="token keyword">pass</span> 

	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span> 
		out <span class="token operator">=</span> x <span class="token operator">+</span> y 
		<span class="token keyword">return</span> out 

	<span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span> 
		dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span> 
		dy <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span> 
		<span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy</code></pre><p id="3fee956c-4990-4c1b-b196-9719bc4a6ad9" class="">덧셈 계층에서는 초기화가 필요 없으니 <strong>init</strong>()에서는 아무 일도 하지 않는다. 덧셈 계층의 forward()에서는 입력받은 두 인수 x, y를 더해서 반환한다. backward()에서는 상류에서 내려온 미분(dout)을 그대로 하류로 흘린다.</p><p id="f11c6c3f-abcf-465b-963c-a647d9226547" class="">이어서 덧셈 계층과 곱셈 계층을 사용하여 사과 2개와 귤 3개를 사는 상황을 구현해 보자!</p><p id="d8e2530a-b959-4d1c-9c09-e856d127dd78" class="">
</p><figure id="63ee4868-c893-4fed-8e9d-3326b5fa3cf6" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2016.png"><img style="width:812px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2016.png"/></a></figure><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="41c4aa6d-b7b7-443f-865a-f8dc259c7935" class="code code-wrap"><code><span class="token comment"># coding: utf-8</span>
<span class="token keyword">from</span> layer_naive <span class="token keyword">import</span> <span class="token operator">*</span>

apple <span class="token operator">=</span> <span class="token number">100</span>
apple_num <span class="token operator">=</span> <span class="token number">2</span>
orange <span class="token operator">=</span> <span class="token number">150</span>
orange_num <span class="token operator">=</span> <span class="token number">3</span>
tax <span class="token operator">=</span> <span class="token number">1.1</span>

<span class="token comment"># layer</span>
mul_apple_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
mul_orange_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
add_apple_orange_layer <span class="token operator">=</span> AddLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
mul_tax_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># forward</span>
apple_price <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple<span class="token punctuation">,</span> apple_num<span class="token punctuation">)</span>  <span class="token comment"># (1)</span>
orange_price <span class="token operator">=</span> mul_orange_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>orange<span class="token punctuation">,</span> orange_num<span class="token punctuation">)</span>  <span class="token comment"># (2)</span>
all_price <span class="token operator">=</span> add_apple_orange_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple_price<span class="token punctuation">,</span> orange_price<span class="token punctuation">)</span>  <span class="token comment"># (3)</span>
price <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>all_price<span class="token punctuation">,</span> tax<span class="token punctuation">)</span>  <span class="token comment"># (4)</span>

<span class="token comment"># backward</span>
dprice <span class="token operator">=</span> <span class="token number">1</span>
dall_price<span class="token punctuation">,</span> dtax <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dprice<span class="token punctuation">)</span>  <span class="token comment"># (4)</span>
dapple_price<span class="token punctuation">,</span> dorange_price <span class="token operator">=</span> add_apple_orange_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dall_price<span class="token punctuation">)</span>  <span class="token comment"># (3)</span>
dorange<span class="token punctuation">,</span> dorange_num <span class="token operator">=</span> mul_orange_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dorange_price<span class="token punctuation">)</span>  <span class="token comment"># (2)</span>
dapple<span class="token punctuation">,</span> dapple_num <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dapple_price<span class="token punctuation">)</span>  <span class="token comment"># (1)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"price:"</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"dApple:"</span><span class="token punctuation">,</span> dapple<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"dApple_num:"</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>dapple_num<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"dOrange:"</span><span class="token punctuation">,</span> dorange<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"dOrange_num:"</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>dorange_num<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"dTax:"</span><span class="token punctuation">,</span> dtax<span class="token punctuation">)</span></code></pre><h1 id="6e9049a7-b88b-4c97-8bfb-9f1907c1fb54" class="">5. 활성화 함수 계층 구현하기</h1><p id="5e09a6ac-6633-473f-ba27-3e90a2b29c52" class="">이제 계산 그래프를 신경망에 적용 해 보자. 우선은 활성화 함수는 ReLU와 Sigmoid 계층을 구현해 보자</p><h2 id="bc5d1032-8d99-4ef5-b682-b652c04f8d4a" class="">5.1 ReLU 계층</h2><p id="da0ce70b-f941-484d-823a-d11fed58e356" class="">활성화 함수로 사용되는 ReLU의 수식은 다음과 같다.</p><p id="1662b445-c1f6-40e6-affb-f7dc501cc249" class="">
</p><figure id="887aa118-3c2b-4da4-9964-9f7cee29e90a" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2017.png"><img style="width:315px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2017.png"/></a></figure><p id="dbd0a4c4-4e48-4a8f-9042-bba9fe88a6cf" class="">위 식에서 x에 대한 y의 미분은 다음과 같다.</p><figure id="240cacbe-989e-4e13-93f8-60e25e1bc99f" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2018.png"><img style="width:339px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2018.png"/></a></figure><p id="372942a7-e3f8-4c13-a087-a7ee97e48f6f" class="">
</p><p id="86803278-4dee-4acb-b913-718ca8da1243" class="">위 식과 같이 순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘린다. 반면  순전파 때 x가 0 이하면 역전파때는 하류로 신호를 보내지 않는다.(0을 보낸다.) 계산 그래프로는 다음과 같이 그릴 수 있다.</p><p id="4bd0d08f-e224-4a7d-93ac-3040ce5cd390" class="">
</p><figure id="9b69171e-502b-44a8-82e7-6268a2a6d50e" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2019.png"><img style="width:784px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2019.png"/></a></figure><p id="2400fd53-4aec-4d78-976c-f75d152086c1" class="">이제 이 ReLU 계층을 구현해 보자! 신경망 계층의 forward()와 backward() 함수는 넘파이 배열을 인수로 받는다고 가정한다. </p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="dde44352-e393-4061-ab13-aa82e34c4e31" class="code code-wrap"><code><span class="token keyword">class</span> <span class="token class-name">Relu</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> x<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
        out<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>

        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dout<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        dx <span class="token operator">=</span> dout

        <span class="token keyword">return</span> dx</code></pre><p id="9e0cc113-f89f-434e-b625-29f284a9e492" class="">ReLU클래스는 mask라는 인스턴스 변수를 가진다. mask는 True/False로 구성된 넘파이 배열로, 순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그외 (0보다 큰 원소)는 False로 유지한다. 예컨데 mask 변수는 다음 예와 같이 True/False로 구성된 넘파이 배열을 유지한다.</p><figure id="8c686003-8612-4495-99e5-757ba087e2d6" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2020.png"><img style="width:302px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2020.png"/></a></figure><p id="a7f7c4c2-04c8-42a4-9033-7ca0e082dfa7" class="">위의 그림과 같이, 순정파 때의 입력값이 0 이면 역전파 때의 값은 0이 되어야 한다. 그래서 역전파 때는 순전파 때 만들어둔 mask를 써서 mask의 원소가 True인 곳에서는 상류에서 전파된 dout을 0으로 설정한다.</p><blockquote id="44d55fd7-7879-4409-ba84-59cedde0430d" class="">ReLU 계층은 전기 회로의 &#x27;스위치&#x27;에 비유할 수 있다. 순전파 때 전류가 흐르고 있으면 스위치를 ON으로 하고, 흐르지 않으면 OFF로 한다. 역전파 때는 스위치가 ON이면 전류가 그대로 흐르고, OFF면 더 이상 흐르지 않는다.</blockquote><p id="0035477c-707a-46c1-ac08-b61b696b08df" class="">
</p><h2 id="a0aaf0f6-c84f-46f7-a9dd-186cec23ec54" class="">5.2 Sigmoid 계층</h2><p id="0cc9a9ba-5f0d-490c-8de1-4a426d61928f" class="">다음은 시그모이드 함수를 구현해보자. 시그모이드 함수는 다음 식을 의미한다.</p><p id="2ef456e8-53f4-4983-8408-2103bd5d2547" class="">
</p><figure id="4faf7049-259a-4f8b-a5ed-bf20005cf21b" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2021.png"><img style="width:310px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2021.png"/></a></figure><p id="50e1c6cd-8d5b-4c16-9ec8-9f456f68724d" class="">위의 식을 계산 그래프로 그리면 다음 그림과 같이 표현할 수 있다.</p><p id="c9d5822b-9cb2-44dc-830b-b6760265993d" class="">
</p><figure id="dde43946-4e57-41b3-a56b-34b7a52d1a1b" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2022.png"><img style="width:800px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2022.png"/></a></figure><p id="bc1d6c97-8933-4144-9e18-2fbb7ed72b00" class="">&#x27;x&#x27;와 &#x27;+&#x27;노드 말고도 &#x27;exp&#x27;와 &#x27;/&#x27;가 새롭게 등장했다. 위 그림과 같이 시그모이드 함수식의 계산은 국소적 계산의 전파로 이뤄진다. 이제 위 그림의 역전파를 하나씩 알아보자</p><ul id="139d7eb2-3415-40d2-8a55-a6e74be124e6" class="bulleted-list"><li>1단계<p id="5059d30b-4bd0-4e84-8abd-dde01d128681" class="">&#x27;/&#x27;노드, 즉 y=1/x를 미분하면 다음 식이 된다.</p><p id="6e551f3c-c4cc-470e-9645-8d7982eb0ab3" class="">
</p><figure id="87cf8650-5710-4603-84ff-e65f0ee38d6a" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2023.png"><img style="width:223px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2023.png"/></a></figure></li></ul><p id="d5fc6fd2-ab09-4a9e-b620-0741a1f11795" class="">위의 식에 따르면 역전파때는 상류에서 흘러온 값에 -y^2를 곱해서 하류로 전달한다.</p><p id="7d6e2b14-0f8b-4d93-a356-5b86accc4140" class="">
</p><figure id="683e9bdc-221d-408a-80fc-3ca23fd0b73a" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2024.png"><img style="width:806px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2024.png"/></a></figure><ul id="39a61b7e-affb-438a-9637-6826543f87d3" class="bulleted-list"><li>2단계<p id="1b305f0d-28c7-46e1-a28e-b8d9fbdba9c4" class="">&#x27;+&#x27;노드는 상류의 값을 여과없이 하류로 내보낸다.</p><p id="024514db-d55a-47cb-b4c6-8c3dcdddcf5b" class="">
</p><figure id="24a6ea15-b1df-4a44-9a57-d67d756895d6" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2025.png"><img style="width:797px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2025.png"/></a></figure><p id="8a54c4a6-848f-4270-8267-108f2fefe8d9" class="">
</p></li></ul><ul id="cc63179b-9261-4231-a5ed-09dc676025ee" class="bulleted-list"><li>3단계<p id="466c5719-ecec-4298-b87b-1c5cf2085dd5" class="">&#x27;exp&#x27;노드는 y=exp(x) 연산을 수행하며 그 미분은 다음과 같다.</p><p id="1b33814f-5b48-4bb5-9cb7-17e8e079889c" class="">
</p><figure id="f34b0221-8b6a-4ec1-a897-e8bf494920ec" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2026.png"><img style="width:246px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2026.png"/></a></figure></li></ul><p id="73c835cc-3bf2-40ff-bae6-b3f68d223707" class="">
</p><figure id="8aad0d42-0a9a-4102-93b4-1b423434d2b9" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2027.png"><img style="width:824px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2027.png"/></a></figure><ul id="64adaeae-2d47-45e5-807d-647b303370b0" class="bulleted-list"><li>4단계<p id="0203ef3f-41d5-49d9-94b1-18da6d2f55d3" class="">&#x27;x&#x27; 노드는 순전파 때의 값을 &#x27;서로 바꿔&#x27; 곱한다. 이 예에서는 -1을 곱하면 된다.</p></li></ul><p id="038498bf-c32a-486e-8c89-0cb4a655596e" class="">
</p><figure id="10b31964-f961-4645-8821-87e196d42729" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2028.png"><img style="width:801px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2028.png"/></a></figure><p id="20884212-1e6e-41be-8eaa-d08ed7ca90d2" class="">이상으로 게산 그래프를 완성 했다.이때, 시그모이드 함수의 역전파는 최종 결과물 dL/dy*y^2*exp(-x)로 묶을 수 있으므로, 다음과 같이 간단하게 표현할 수 있다.</p><p id="316f1dd4-4cb9-4b36-b120-6312260a6328" class="">
</p><figure id="1bc813da-36ed-426a-a137-c0f0d758eadf" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2029.png"><img style="width:613px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2029.png"/></a></figure><p id="f03362a3-5161-4ce4-bcec-78701c269fc1" class="">결과는 같으나 노드의 그룹화를 통해 입력과 출력에 집중할 수 있다.</p><p id="15dd1955-2742-4d1b-a303-4e54fb4891d4" class="">또한 dL/dy*y^2*exp(-x)은 다음처럼 정리해서 쓸 수 있다.</p><p id="0422d63a-3736-4a28-8339-6f64845c0d59" class="">
</p><figure id="23aa195e-6159-4e18-aae0-d8321baeae5f" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2030.png"><img style="width:612px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2030.png"/></a></figure><p id="8cfda0cf-709f-4d17-afad-212c744af02f" class="">이처럼 Sigmoid 계층의 역전파는 순전파의 출력(y)만으로 계산할 수 있다.</p><p id="4f76fbe7-b264-4b8b-80d6-b31a61803cec" class="">
</p><figure id="7de0c079-a99d-430b-b76c-efe73dfd5a17" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2031.png"><img style="width:579px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2031.png"/></a></figure><p id="a4ef59ab-7e01-489a-985b-60b3d83a88a6" class="">그럼 이제 sigmoid 계층을 파이썬으로 구현해보자</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="de81a84c-1463-46ea-8ae6-2cdb45483adc" class="code code-wrap"><code><span class="token keyword">class</span> <span class="token class-name">Sigmoid</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> out
        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>out<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>out

        <span class="token keyword">return</span> dx</code></pre><p id="4ff0d1e4-10e3-4b9c-a010-2ac6c09bf38a" class="">이 구현에서는 순전파의 출력을 인스턴스 변수 out에 보관했다가 역전파 계산 때 그 값을 사용한다.</p><h1 id="f4677904-0240-4db4-b9bd-182905b56fac" class="">6. Affine/Softmax 계층 구현하기</h1><p id="82cb17a8-6a22-45aa-a5f0-eb917fed49e2" class="">
</p><h2 id="672b5511-28d1-43b5-b160-0e7d06ca6fe5" class="">6.1 Affine 계층</h2><p id="7bc5b728-db91-4402-8b2c-020fb519c3cf" class="">신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 곱(넘파이에서는 np.dot())을 사용했다.</p><p id="ee154dc4-dcdb-4e14-9178-8e8a25855ec1" class="">예시코드를 보자</p><p id="1205585a-cf08-4a19-b510-786e37dfabe2" class="">
</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="9ca1ae84-a484-4d9e-9cd6-b9df446ab9b6" class="code code-wrap"><code>X <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
W <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>

X<span class="token punctuation">.</span>shape <span class="token comment"># (2,)</span>
W<span class="token punctuation">.</span>shape <span class="token comment"># (2,3)</span>
B<span class="token punctuation">.</span>shpae <span class="token comment"># (3,)</span>

Y <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">+</span>  B</code></pre><p id="4b1656ba-3e31-4072-a08d-5741982d7451" class="">예를 들어 다음과 같은 코드를 계산 그래프로 나타내면 다음과 같다.</p><p id="839fc965-f62d-4d8d-886b-52a4f3a827fe" class="">
</p><figure id="19f9b097-93d6-4ec2-9177-7457030dd313" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2032.png"><img style="width:806px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2032.png"/></a></figure><blockquote id="24a379fe-aca2-47af-9f33-50a37dba79b2" class="">신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서는 어파인 변환이라고 한다. 그래서 어파인 변환을 수행하는 처리를 &#x27;Affine 계층&#x27;이라고 정의했다.</blockquote><p id="1a4477a2-ca7a-49c6-a040-000d8288afa7" class="">이제 위 그림에 대한 역전파를 구해보자. 행렬을 이용한 역전파도 행렬의 원소마다 전개해보면 스칼라값을 사용한 지금까지의 계산그래프와 같은 순서로 생각할 수 있다.</p><p id="7077e628-64bd-4453-b0db-584c22178335" class="">
</p><figure id="75450259-84c6-4e8d-9876-4e6632677a3d" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2033.png"><img style="width:809px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2033.png"/></a></figure><p id="63bcc6e5-11f6-4627-960a-88ea3e955f9f" class="">W^T의 T는 전치행렬을 뜻한다. 전치행렬은 W의 (i, j) 위치의 원소를 (j, i) 위치로 바꾼 것을 말한다. 수식으로는 다음과 같다.</p><p id="0d8b8dc6-2aef-429d-aa1b-a628f3151393" class="">
</p><figure id="29673104-53cd-4e45-9b3f-7b2e4ecae3c4" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2034.png"><img style="width:365px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2034.png"/></a></figure><p id="21274d43-a0c3-4d81-97e4-356afa5a721b" class="">위 식을 바탕으로 한 역전파는 다음과 같이 구할 수 있다.</p><p id="2cb815a3-23af-4cc4-ba5a-26b76f75db49" class="">
</p><figure id="75be7a16-9d70-4a07-b17c-bc96d4b5f4d9" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2035.png"><img style="width:801px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2035.png"/></a></figure><p id="91ed028e-ce50-4ef5-b41a-2bfb3a06bdac" class="">
</p><figure id="6dc673b6-0c87-4861-b5ee-120c02e59612" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2036.png"><img style="width:497px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2036.png"/></a></figure><p id="38fce755-ba2d-4800-8c79-c2f6f9be533d" class="">이때 행렬 곱의 역전파는 행렬의 대응하는 차원의 원소 수가 일치하도록 잘 조립해주어야 한다.</p><p id="70b7884a-028d-4ad1-815c-b7536061aea8" class="">
</p><figure id="340335ac-6cd9-4759-9413-9394ef132ea2" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2037.png"><img style="width:795px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2037.png"/></a></figure><p id="7ac4256c-1e3d-4efc-8aa1-98998dc51838" class="">
</p><h2 id="6d928f4c-b71b-4455-bcc0-0ed5704b93d1" class="">6.2 배치용 Affine 계층</h2><p id="18db7930-fa32-4f81-b20b-41eebe0c2a1d" class="">지금까지의 Affine 계층은 입력 데이터로 X하나만을 고려한 것 이었다. 이번 절에서는 데이터 N개를 묶어 순전파하는 경우, 즉 배치용(데이터 묶음) Affine 계층을 생각해 보자.</p><p id="46e0818c-5d6f-4b14-b40b-aca23b0432e3" class="">
</p><figure id="f3890a99-a1cc-4bc1-b3ae-71d217400b55" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2038.png"><img style="width:807px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2038.png"/></a></figure><p id="74708b57-aaf5-496d-9c09-e3bd03b73c36" class="">기존과 다른 부분은 입력인 X의 형상이 (N, 2)가 된 것뿐이다. 그 뒤로는 지금까지와 같은 계산 그래프의 순서를 따라 순순히 행렬 계산을 하게 된다.</p><p id="5d59555c-509d-4276-af87-47d8529cf161" class="">
</p><p id="5b1e64ac-179c-4505-953e-c42ce8465ccc" class="">편향을 더할 때도 주의해야 한다. 순전파 때의 편향 덧셈은 X*W에 대한 편향이 각 데이터에 더해진다. 예를 들어 N=2(묶음이 2개짜리)로 한경우 편향은 그 두 데이터 각각에 더해진다.</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="e39c13aa-43cb-4cae-b1b6-4857213118e3" class="code code-wrap"><code>X_dot_W <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># X_dot_W + B</span>
<span class="token comment">#[[1, 2, 3], [11, 12, 13]])</span></code></pre><p id="64303980-52c2-4c25-a8c6-eaa47b3f0f34" class="">순전파의 편향 덧셈은 각각의 데이터에 더해진다 그래서 역전파때는 각 데이터의 역전파 값이 편향의 원소에 모여야한다.</p><p id="2dde3561-ea52-49f9-b283-6a46552a5370" class="">
</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="223d6633-8dae-4225-9260-0c3cebbf4c53" class="code code-wrap"><code>dY <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
dB <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dY<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment">#dB -> [5, 7, 9]</span></code></pre><p id="64819f71-aee6-48d2-9406-ff06eea2a169" class="">편향의 역전파는 그 두 데이터에 대한 미분을 데이터마다 더해서 구한다. 그래서 np.sum()에서 0번째 축에 대해 (axit = 0) 총합을 구한다.</p><p id="bd5cf642-4b9a-4c28-b8b3-504a2f1d3020" class="">
</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="7c2f1fba-f2bf-4011-a2c3-ed19b5e0d98b" class="code code-wrap"><code><span class="token keyword">class</span> <span class="token class-name">Affine</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span>W
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b
        
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>original_x_shape <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token comment"># 가중치와 편향 매개변수의 미분</span>
        self<span class="token punctuation">.</span>dW <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>db <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>

        self<span class="token punctuation">.</span>original_x_shape <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x

        out <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b

        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dout<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dW <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>db <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dout<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
				<span class="token keyword">return</span> dx</code></pre><h2 id="88605a44-20c9-4b0d-9d71-72a2845624ab" class="">6.3 Softmax-with-Loss 계층</h2><p id="e650179d-4eef-4029-9311-96ba861a4631" class="">마지막에서 출력층에서 사용하는 소프트맥스 함수에 대해 알아보자. 소프트맥스 함수는 입력 값을 정규화하여 출력한다. 예를 들어 손글씨 숫자 인식에서의 출력은 다음과 같다.</p><p id="f27ac7bc-7a1f-4020-9bf8-7279768519b3" class="">
</p><figure id="9d593c5e-8b4b-49d2-9dd1-9a408c196fbf" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2039.png"><img style="width:809px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2039.png"/></a></figure><p id="549c2546-6fe0-4a7b-960c-e014b24232fe" class="">위와 같이 Softmax계층은 입력 값을 정규화(출력의 합이 1이 되도록 변형)하여 출력한다. 손글씨 숫자는 가짓수가 10개 (10클래스 분류)이므로 Softmax계층의 입력은 10개가 된다.</p><blockquote id="1eeed2d3-c84a-4aab-8145-64a854817945" class="">신경망에서 수행하는 작업은 학습과 추론 두가지이다. 추론할 때는 일반적으로 Softmax 계층을 사용하지 않는다. 예를 들어 위에서 신경망은 추론할 때 마지막 Affine 계층의 출력을 인식 결과로 이용한다. 또한, 신경망에서 정규화하지 않는 출력 결과(Affine 계층의 출력)를 점수(Score)라고 한다. 즉, 신경망 추론에서 답을 하나만 내는 경우에는 가장높은 점수만 알면되니 Softmax 계층은 필요없다는 것이다. 반면 신경망을 학습할때는 Softmax계층이 필요하다.</blockquote><p id="6ff5b245-1c0b-4854-9fe9-b749c07af806" class="">
</p><p id="b9e099f2-5d8c-49e5-b201-00d591f47af5" class="">이제 손실함수인 교차 엔트로피 오차도 포함하여 &#x27;softmax-with-Loss계층&#x27;이라는 이름으로 구현해보자. 계산 그래프는 다음과 같다.</p><figure id="fa3bfc33-e104-4fab-bf4b-31d5b74c6653" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2040.png"><img style="width:797px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2040.png"/></a></figure><p id="8a0be1c2-887f-410f-8467-adbd053989d2" class="">위의 계산 그래프는 다음과 같이 간소화 할 수 있다.</p><p id="22273a13-4f6b-4ac7-a316-e044e6bc8e35" class="">
</p><figure id="7b6eb3be-b656-4c55-bd51-e0b8e345f4b1" class="image"><a href="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2041.png"><img style="width:854px" src="CHAPTER%205%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1%E1%84%87%E1%85%A5%E1%86%B8%20193a117513774cd4acca55201351c79a/Untitled%2041.png"/></a></figure><p id="42262765-8e0d-49c6-8909-8c2759b32716" class="">위의 계산 그래프에서 소프트맥스 함수는 &#x27;Softmax&#x27; 계층으로 교차 엔트로피 오차는 &#x27;Cross Entropy Error&#x27; 계층으로 표시했다. 여기에서는 3클래스 분류를 가정하고 이전 계층에서 3개의 입력(점수)을 받는다. 그림과 같이 Softmax계층은 입력(a1, a2, a3)를 정규화하여 (y1, y2, y3)를 출력한다. Cross Entropy Error 계층은 Softmax의 출력(y1, y2, y3)와 정답 레이블 (t1, t2, t3)를 받고, 이 데이터들로부터 손실 L을 출력한다.</p><p id="768a01b0-3d56-4ecd-afbb-8ec1479f0bf9" class="">
</p><p id="cc57fa03-07f7-4e5d-b9e6-5f01050c66a1" class="">여기서 주목할 것은 역전파의 결과이다. Softmax 계층의 역전파는 (y1-t1, y2-t2, y3-t3)라는 말끔한 결과를 내놓고 있다. (y1, y2, y3)는 Softmax 계층의 출력이고 (t1, t2, t3)는 정답 레이블이므로 (y1-t1, y2-t2, y3-y3)라는 말끔한 결과를 내놓고 있다. (y1, y2, y3)는 Softmax 계층의 출력이고 (t1, t2, t3)는 정답 레이블 이므로 (y1-t1, y2-t2, y3-t3)는 Softmax 계층의 출력과 정답 레이블의 차분인 것이다. 신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해지는 것이다. 이것은 신경망 학습의 중요한 성질이다.</p><p id="b8f90430-efd1-4d8c-b2eb-a062dc04d9c3" class="">
</p><p id="d4819f6e-4641-4743-bb1a-44e59b6c63a3" class="">그런데 신경항 학습의 목적은 신경망의 출력이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는것 이었다. 그래서 신경망의 출력과 정답 레이블의 오차를 효율적으로 앞 계층에 전달해야 한다. 앞의 (y1-t1, y2-t2, y3-t3)라는 결과는 바로 Softmax 계층의 출력과 정답 레이블의 차이로 신경망의 현재 출력과 정답 레이블 오차를 있는 그대로 드러내고 있다.</p><blockquote id="5e311f38-c308-481b-abb7-70e333b62d5a" class="">소프트맥스 손실함수로 교차 엔트로피 오차를 사용하니 역전파가 (y1-t1, y2-t2, y3-t3)로 말끔히 떨어진다.</blockquote><p id="74e544b2-c6d7-4ea8-bd7b-5a1f4dbcc467" class="">예를 들어 정답 레이블이 (0,1,0)일 떄 Softmax 계층이 (0.3, 0.2, 0.5)를 출력했다고 해보자. 정답 레이블을 보면 정답의 인덱스는 1이다. 그런데 출력에서는 이때의 확률이 겨우 0.2(20%)라서, 이 시점의 신경망은 제대로 인식하지 못하고 있다. 이 경우 Softmax계층의 역전파는 (0.3, -0.8, 0.5)라는 커다란 오차를 전파한다. 결과적으로 Softmax 계층의 앞 계층들은 그 큰 오차로부터 큰 깨달음을 얻게 된다.</p><p id="63b2b884-3403-4d2e-862f-09dcea6dc724" class="">
</p><p id="5c522eec-244b-411a-a7ac-4b0974cf87b1" class="">만약 정답 레이블은(0, 1, 0)이고 Softmax계층이 (0.01, 0.99, 0)을 출력한 경우가 있다고 해보자. 이 경우 Softmax 계층의 역전파가 보내는 오차는 비교적 작은(0.01, -0.01, 0) 이다</p><p id="f9af1126-74a3-4ff9-b7a9-dc90de8c7a05" class="">
</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="0ca06135-bd39-473f-b511-f256c338edb3" class="code code-wrap"><code><span class="token keyword">class</span> <span class="token class-name">SoftmaxWithLoss</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>loss <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># 손실</span>
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># softmax의 출력</span>
        self<span class="token punctuation">.</span>t <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># 정답레이블(원-핫 벡터)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>t <span class="token operator">=</span> t
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>loss <span class="token operator">=</span> cross_entropy_error<span class="token punctuation">(</span>self<span class="token punctuation">.</span>y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>t<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>loss

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_size <span class="token operator">=</span> self<span class="token punctuation">.</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        dx <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>y <span class="token operator">-</span> self<span class="token punctuation">.</span>t<span class="token punctuation">)</span> <span class="token operator">/</span> batch_size
				<span class="token keyword">return</span> dx</code></pre><p id="6afa1215-7d2d-49d0-8325-1e30863510bf" class="">
</p><h1 id="d678d0e3-dd1d-4b76-b120-04d29aa11b19" class="">7. 오차역전파법 구현하기</h1><p id="deee13b0-08e3-4c58-9ccb-123f63adf970" class="">
</p><h2 id="ef4a31c9-c7bb-4d90-83cb-e79e7cdd4a8a" class="">7.1 신경망 학습의 전체 그림</h2><p id="7ad0e64d-2c95-4e38-b371-57b62029efbf" class="">구체적인 구현에 들어가기 전에 신경망 학습의 전체 그림을 복습해 보자.</p><p id="49c4851e-bd0e-49f6-beef-b4b6b2159048" class="">
</p><ul id="31a05112-45d9-427e-b3d3-2cc365369f23" class="bulleted-list"><li>전제<p id="fa25cf9a-6e4f-491c-b390-f4eb1db3efb1" class="">신경망에는 적용 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 &#x27;학습&#x27;이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.</p><p id="d2765fc2-683f-4320-a9b5-73a318ff6a5a" class="">
</p></li></ul><ul id="8fad8a1e-46bb-454d-8d35-ccbe14c0975c" class="bulleted-list"><li>1단계-미니배치<p id="12e4879f-0827-45df-b4ee-95d88ed921a0" class="">훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표이다.</p><p id="216c2c96-fa3c-4964-b248-bc7931e126d7" class="">
</p></li></ul><ul id="85021f70-007a-4b03-8ffe-1d6d94069d9f" class="bulleted-list"><li>2단계-기울기 산출<p id="51c86651-607e-48a7-b4d4-ff3aa23bec34" class="">미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개 변수의 기울기를 구한다. 기울기는 손실함수의 값을 가장 작게 하는 방향을 제시한다.</p><p id="a666e33e-3b56-4774-8a5d-e26b61bf3c43" class="">
</p></li></ul><ul id="6e489a80-3b03-4c8d-91bc-c6fb7964d007" class="bulleted-list"><li>3단계-매개변수 갱신<p id="35fe42c5-b403-465c-abae-4e2c21e1c2d5" class="">가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.</p><p id="83fc2982-1592-4409-878c-7d3c25228819" class="">
</p></li></ul><ul id="f8e4c0a1-36f6-4c49-9586-7533666eb34b" class="bulleted-list"><li>4단계-반복<p id="cc988c2a-224d-4b66-acd5-fd03a91dd2c6" class="">1~3단계를 반복한다.</p><p id="e84d79c4-6465-4507-a942-a541ecf1a546" class="">지금까지 설명한 오차역전파법이 등장하는 단계는 두 번째인 &#x27;기울기 산출&#x27;이다. 오차역전파법을 이용하면 느린 수치 미분과 달리 기울기를 효율적이고 빠르게 구할 수 있다.</p></li></ul><h2 id="a3d41d15-5a61-408e-96a9-8ab1c5334f44" class="">7.2 오차역전파법을 적용한 신경망 구현하기</h2><p id="053e3bdd-31c2-4df9-92ee-b5245169cbea" class="">2층 신경망을 TwoLayerNet 클래스로 구현한다.</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="5d9f14a9-a054-41f9-bead-6a0b593406ea" class="code code-wrap"><code><span class="token comment"># coding: utf-8</span>
<span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>pardir<span class="token punctuation">)</span>  <span class="token comment"># 부모 디렉터리의 파일을 가져올 수 있도록 설정</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> common<span class="token punctuation">.</span>layers <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> common<span class="token punctuation">.</span>gradient <span class="token keyword">import</span> numerical_gradient
<span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict


<span class="token keyword">class</span> <span class="token class-name">TwoLayerNet</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> weight_init_std <span class="token operator">=</span> <span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 가중치 초기화</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span> 
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>output_size<span class="token punctuation">)</span>

        <span class="token comment"># 계층 생성</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> Affine<span class="token punctuation">(</span>self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Relu1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> Relu<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> Affine<span class="token punctuation">(</span>self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>lastLayer <span class="token operator">=</span> SoftmaxWithLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> x
        
    <span class="token comment"># x:입력 데이터, t:정답레이블</span>
    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>lastLayer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">accuracy</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> t<span class="token punctuation">.</span>ndim <span class="token operator">!=</span> <span class="token number">1</span> <span class="token punctuation">:</span> t <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        accuracy <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>y <span class="token operator">==</span> t<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">float</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> accuracy
        
    <span class="token comment"># x:입력 데이터 t:정답 레이블</span>
    <span class="token keyword">def</span> <span class="token function">numerical_gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss_W <span class="token operator">=</span> <span class="token keyword">lambda</span> W<span class="token punctuation">:</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
        
        grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> grads
        
    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># forward</span>
        self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        <span class="token comment"># backward</span>
        dout <span class="token operator">=</span> <span class="token number">1</span>
        dout <span class="token operator">=</span> self<span class="token punctuation">.</span>lastLayer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>
        
        layers <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        layers<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>
            dout <span class="token operator">=</span> layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>

        <span class="token comment"># 결과 저장</span>
        grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dW<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>db
        grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dW<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>db

        <span class="token keyword">return</span> grads</code></pre><p id="4abb173f-d084-494b-9c43-7df2b6cde2fc" class="">계층 생성 부분을 잘 보자 신경망의 계층을 OrderedDict()에 보관하는 점이 중요하다 OrderedDict는 순서가 있는 딕셔너리다.</p><p id="c49a13b0-2b19-403e-b6a2-75debc350a88" class="">그래서 순전파 때는 추가한 순서대로 각 계층을 forward()메서드를 호출하기만 하면 처리가 완료된다.</p><p id="b5b54e0c-4af8-436b-9338-603c405d1b90" class="">마찬가지로 역전파 때는 계층을 반대 순서로 호출하기만하면 된다.</p><p id="04af20fc-0907-4785-8f5b-5e709a01398e" class="">이처럼 신경망을 &#x27;계층&#x27;으로 모듈화해서 구현한 효과는 아주 크다. 예컨데 5층 10층 100층 과 같이 깊은 신경망을 만들고 싶다면 단순히 필요한 만큼 계층을 더 추가 하면 된다.</p><p id="b3dd6c18-1278-4ce3-9b83-8935ecd5fb3a" class="">
</p><h2 id="9bae0649-7218-4739-a41c-70bd1fbe2cd3" class="">7.3 오차역전파법으로 구한 기울기 검증하기</h2><p id="5983d7ef-e9f7-4efe-a74f-83ff2173e643" class="">수치 미분은 느리지만 구현이 간단하다 그래서 오차역전파법으로 구한 기울기가 일치함을 체크함으로써 계산의 정확성을 확인 할 수 있다.</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="45adc69c-3f24-4c9c-ac97-afcd945904c8" class="code code-wrap"><code><span class="token comment"># coding: utf-8</span>
<span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>pardir<span class="token punctuation">)</span>  <span class="token comment"># 부모 디렉토리의 파일을 가져올 수 있도록 설정</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist
<span class="token keyword">from</span> two_layer_net <span class="token keyword">import</span> TwoLayerNet

<span class="token comment"># 데이터읽기</span>
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>
t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>

grad_numerical <span class="token operator">=</span> network<span class="token punctuation">.</span>numerical_gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
grad_backprop <span class="token operator">=</span> network<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
<span class="token comment"># 각 가중치의 절대 오차의 평균을 구한다.</span>
<span class="token keyword">for</span> key <span class="token keyword">in</span> grad_numerical<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    diff <span class="token operator">=</span> np<span class="token punctuation">.</span>average<span class="token punctuation">(</span> np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>grad_backprop<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-</span> grad_numerical<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>key <span class="token operator">+</span> <span class="token string">":"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>diff<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p id="1680b21d-9b8b-4724-aaef-ca3cb0d827af" class="">결과는 다음과 같다.</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="18519754-9bf0-44b6-8685-773d684d9528" class="code code-wrap"><code>W1<span class="token punctuation">:</span><span class="token number">4.992444822296182e-10</span> 
b1<span class="token punctuation">:</span><span class="token number">2.7775308568029376e-09</span> 
W2<span class="token punctuation">:</span><span class="token number">6.1427267257827926e-09</span> 
b2<span class="token punctuation">:</span><span class="token number">1.4103044333468872e-07</span></code></pre><p id="e0e07fad-3226-411f-8db5-fb690505ca7c" class="">각 가중치 매개변수의 차이의 절대값을 구하고 이를 평균한 값을 구한 것이다.</p><h2 id="d5a384dd-e89f-4783-94cb-988bc83cda4e" class="">7.4 오차역전파법을 사용한 학습 구현하기</h2><p id="c68f21f3-1c43-48a0-9792-1c256caf7fc0" class="">마지막으로 오차역전파법을 사용한 신경망 학습을 구현해 보자. 지금까지와 다른 부분은 기울기를 오차역전파법으로 구현한다는 것 뿐이다.</p><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css')</style><pre id="37f47aae-a74d-4e1e-aa6c-7a5b75ebb88c" class="code code-wrap"><code><span class="token comment"># coding: utf-8</span>
<span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>pardir<span class="token punctuation">)</span>

<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist
<span class="token keyword">from</span> two_layer_net <span class="token keyword">import</span> TwoLayerNet

<span class="token comment"># data load</span>
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

iters_num <span class="token operator">=</span> <span class="token number">10000</span>
train_size <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
batch_size <span class="token operator">=</span> <span class="token number">100</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>

train_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
train_acc_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
test_acc_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

iter_per_epoch <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>train_size <span class="token operator">/</span> batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>iters_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>train_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
    t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
    
    <span class="token comment"># 기울기 계산</span>
    <span class="token comment">#grad = network.numerical_gradient(x_batch, t_batch) # 수치미분방식</span>
    grad <span class="token operator">=</span> network<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span> <span class="token comment"># 오차역전파법 방식(훨씬 빠르다)</span>
    
    <span class="token comment"># 갱신</span>
    <span class="token keyword">for</span> key <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'W1'</span><span class="token punctuation">,</span> <span class="token string">'b1'</span><span class="token punctuation">,</span> <span class="token string">'W2'</span><span class="token punctuation">,</span> <span class="token string">'b2'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        network<span class="token punctuation">.</span>params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> grad<span class="token punctuation">[</span>key<span class="token punctuation">]</span>
    
    loss <span class="token operator">=</span> network<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
    train_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> i <span class="token operator">%</span> iter_per_epoch <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        train_acc <span class="token operator">=</span> network<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span>
        test_acc <span class="token operator">=</span> network<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span>
        train_acc_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_acc<span class="token punctuation">)</span>
        test_acc_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>test_acc<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>train_acc<span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span></code></pre><h1 id="e049f463-69fa-401a-83d4-48bdc0ec47b3" class="">8. 정리</h1><ul id="175672be-3121-4800-ad29-5b941d65ceb4" class="bulleted-list"><li>계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.</li></ul><ul id="44e1674e-e10e-4e5d-b888-05f5fe425ef6" class="bulleted-list"><li>계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.</li></ul><ul id="6a360dc7-8085-408d-823a-6ffdbe77e76b" class="bulleted-list"><li>계산 그래프의 순정파는 통상의 계산을 수행한다. 한편, 계산 그래프의 역전파로는 각 노드의 미분을 구할 수 있다.</li></ul><ul id="67dc25d7-5da3-40c1-a89d-d692fa5c7095" class="bulleted-list"><li>신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다.(오차역전파법)</li></ul><ul id="a218468e-4a16-4fd8-a3fc-cd881fff8a32" class="bulleted-list"><li>수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수있다.(기울기 확인)</li></ul></div></article></body></html>